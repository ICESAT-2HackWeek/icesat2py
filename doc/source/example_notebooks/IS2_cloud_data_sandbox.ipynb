{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICESat-2 AWS cloud data access\n",
    "This notebook ({nb-download}`download <IS2_cloud_data_access.ipynb>`) illustrates the use of icepyx for accessing ICESat-2 data currently available through the AWS (Amazon Web Services) us-west2 hub s3 data bucket.\n",
    "\n",
    "## Notes\n",
    "1. ICESat-2 data became publicly available on the cloud on 29 September 2022. Thus, access methods and example workflows are still being developed by NSIDC, and the underlying code in icepyx will need to be updated now that these data (and the associated metadata) are available. We appreciate your patience and contributions (e.g. reporting bugs, sharing your code, etc.) during this transition!\n",
    "2. This example and the code it describes are part of ongoing development. Current limitations to using these features are described throughout the example, as appropriate.\n",
    "3. You **MUST** be working within an AWS instance. Otherwise, you will get a permissions error.\n",
    "4. Authentication is still more steps than we'd like. We're working to address this - let us know if you'd like to join the conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import icepyx as ipx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add a new cell block for testing pushing to GH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pwd\n",
    "%pip install ../../../\n",
    "%pip install -e../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import icepyx as ipx\n",
    "%autoreload 2\n",
    "\n",
    "print(ipx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Create an icepyx Query object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding box\n",
    "# \"producerGranuleId\": \"ATL03_20191130221008_09930503_004_01.h5\",\n",
    "short_name = 'ATL03'\n",
    "spatial_extent = [-45, 58, -35, 75]\n",
    "date_range = ['2019-11-30','2019-11-30'] ### NOTE THESE PARAMETERS BREAK FOR v006!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=ipx.Query(short_name, spatial_extent, date_range, version=\"005\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Get the granule s3 urls\n",
    "You must specify `cloud=True` to get the needed s3 urls.\n",
    "This function returns a list containing the list of the granule IDs and a list of the corresponding urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gran_ids = reg.avail_granules(ids=True, cloud=True)\n",
    "gran_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3urls = gran_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log in to Earthdata and generate an s3 token\n",
    "You can use icepyx's existing login functionality to generate your s3 data access token, which will be valid for *one* hour.\n",
    "\n",
    "We currently do not have this set up to automatically renew, but [earthaccess](), which icepyx will soon be adopting for authentication, is working on handling the limits imposed by expiring s3 tokens. If you're interested in working on helping icepyx and NSIDC (and DAACs more broadly) address these challenges, please get in touch or submit a PR. Documentation/example testers are always appreciated (so you don't have to understand the code)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.earthdata_login(s3token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your s3 access using your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = earthaccess.get_s3fs_session(daac='NSIDC', provider=reg._s3login_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg._s3login_credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## H5Coro Playtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (1) import\n",
    "from h5coro import h5coro, s3driver, filedriver\n",
    "\n",
    "# (2) configure\n",
    "h5coro.config(errorChecking=True, verbose=False, enableAttributes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3url = s3urls[0]\n",
    "print(s3url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what's currently in the variables module\n",
    "%time\n",
    "\n",
    "import h5py\n",
    "\n",
    "# in order to treat the inputs \"like\" files\n",
    "fileset = s3.open(s3url)\n",
    "\n",
    "_avail = []\n",
    "\n",
    "def visitor_func(name, node):\n",
    "    if isinstance(node, h5py.Group):\n",
    "        # node is a Group\n",
    "        pass\n",
    "    else:\n",
    "        # node is a Dataset\n",
    "        _avail.append(name)\n",
    "\n",
    "with h5py.File(fileset, \"r\") as h5f:\n",
    "    h5f.visititems(visitor_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (3) create\n",
    "\n",
    "my_bucket = 's3'\n",
    "path_to_hdf5_file = 'nsidc-cumulus-prod-protected/ATLAS/ATL03/005/2019/11/30/ATL03_20191130112041_09860505_005_01.h5'\n",
    "\n",
    "# h5obj = h5coro.H5Coro(f'{s3url}',\n",
    "h5obj = h5coro.H5Coro(f'{my_bucket}/{path_to_hdf5_file}', \n",
    "                      s3driver.S3Driver,\n",
    "                     credentials={\"aws_access_key_id\":reg._s3login_credentials[\"accessKeyId\"],\n",
    "                                 \"aws_secret_access_key\":reg._s3login_credentials[\"secretAccessKey\"],\n",
    "                                 \"aws_session_token\":reg._s3login_credentials[\"sessionToken\"], })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) read\n",
    "datasets = [{'dataset': '/path/to/dataset1', 'startrow': 0, 'numrows': h5coro.ALL_ROWS},\n",
    "            {'dataset': '/path/to/dataset2', 'startrow': 324, 'numrows': 50}]\n",
    "h5obj.readDatasets(datasets=datasets, block=True)\n",
    "\n",
    "# (5) display\n",
    "for dataset in h5obj:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to make data read-in work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to treat the inputs \"like\" files\n",
    "fileset = [s3.open(file) for file in s3urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(fileset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_ds = xr.open_mfdataset(fileset,\n",
    "                           combine='by_coords',\n",
    "                           mask_and_scale=True,\n",
    "                           decode_cf=True,\n",
    "                           chunks='auto')\n",
    "ssh_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: revisit how want to accept s3 inputs. Can users use the filename matching, or should they provide an explicit list of urls? Based on that, will need to either use _check_source_for_pattern or add antoher fn that just returns the right filelist and handles credentialling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = 's3://nsidc-cumulus-prod-protected/ATLAS/ATL03/005/2019/11/30/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"ATL{product:2}_{datetime:%Y%m%d%H%M%S}_{rgt:4}{cycle:2}{orbitsegment:2}_{version:3}_{revision:2}.h5\"\n",
    "# reader = ipx.Read(path_root, \"ATL03\", pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ipx.Read(fileset[0], \"ATL03\", pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader._source_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps: see also note above\n",
    "\n",
    "See notes and warnings within code and here to address the shortcuts taken to get to the load() step to try it\n",
    "(e.g.\n",
    "\n",
    "    let source be the s3url list, and then don't require the pattern input (or let it default?)\n",
    "    update the pattern check function to actually check the urls (and return false otherwise)... lines 391... may need to write a new fnmatch function as for if/elif above\n",
    "    *** can still only load data from one product type at a time, so the pattern check will accomplish this!\n",
    ")\n",
    "\n",
    "17 May 2023\n",
    "some work will be needed on the intake/catalog side to make this work for data read-in and merging.\n",
    "This is likely a good space for Rachel to focus (perhaps first adopting earthaccess under the hood wherever possible?)\n",
    "For the purposes of the GeoSMART tutorial and ATL11 cloud read-in, we're going to have to stick with datatree or a more manual approach in the short term...\n",
    "                                                                                                                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader.vars.avail()  # NOTE THIS WAS REALLY SLOW!!\n",
    "#What is best approach to letting the user know what's available? \n",
    "#Should there be per-product default lists for the cloud, since all vars are always available?\n",
    "# if bloating icepyx itself is in question, perhaps they could be generated in a separate repo and update (e.g. monthly)\n",
    "#by a cron job. then behind the scenes we can just grab the list for the data product of interest,\n",
    "# since if the user is on the cloud (and authenticated) they clearly have internet access..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.vars.append(var_list=['dist_ph_along','dist_ph_across','signal_conf_ph','h_ph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.vars.wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader._source_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = reader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an s3 url and access the data\n",
    "Data read in capabilities for cloud data are coming soon in icepyx (targeted Winter 2022-2023). Stay tuned and we'd love for you to join us and contribute!\n",
    "\n",
    "**Note: If you get a PermissionDenied Error when trying to read in the data, you may not be sending your request from an AWS hub in us-west2. We're currently working on how to alert users if they will not be able to access ICESat-2 data in the cloud for this reason**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first index, [1], gets us into the list of s3 urls\n",
    "# the second index, [0], gets us the first entry in that list.\n",
    "s3url = gran_ids[1][0]\n",
    "# s3url =  's3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2019/11/30/ATL03_20191130221008_09930503_004_01.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f = h5py.File(fs.open(s3url,'rb'),'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credits\n",
    "* notebook by: Jessica Scheick\n",
    "* source material: [is2-nsidc-cloud.py](https://gist.github.com/bradlipovsky/80ab6a7aff3d3524b9616a9fc176065e#file-is2-nsidc-cloud-py-L28) by Brad Lipovsky"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
