{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access and Visualize ICESat-2 ATL08 Data\n",
    "### Data Query, Download, and Visualization Example Notebook\n",
    "This notebook illustrates the use of `icepyx` for ICESat-2 data access and download from the NASA NSIDC DAAC (NASA National Snow and Ice Data Center Distributed Active Archive Center) as part of a basic data analysis workflow.\n",
    "Although it uses ATL08 as the dataset of interest, `icepyx` provides access to all ICESat-2 datasets.\n",
    "More detailed example notebooks ([DataAccess1](https://github.com/icesat2py/icepyx/blob/development/examples/ICESat-2_DAAC_DataAccess_Example.ipynb); [DataAccess2_Subsetting](https://github.com/icesat2py/icepyx/blob/development/examples/ICESat-2_DAAC_DataAccess2_Subsetting.ipynb)) demonstrate in greater detail the data access, ordering, and subsetting options available when obtaining data using `icepyx`.\n",
    "\n",
    "#### Objectives\n",
    "1. Declare search parameters\n",
    "2. Create an icepyx query object\n",
    "3. Visualize the area of interest and data prior to ordering\n",
    "4. Order and download ICESat-2 data\n",
    "5. Quickly open and visualize the downloaded data\n",
    "\n",
    "#### Credits\n",
    "* tutorial by: Jessica Scheick, for the May 2021 ICESat-2 Data Users Quarterly Call\n",
    "* source material and coauthors: [icepyx examples](https://icepyx.readthedocs.io/en/latest/getting_started/example_link.html) and references therein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up your computing environment\n",
    "\n",
    "If you have not already installed `icepyx` and the libraries it depends on, please see our [installation instructions](https://icepyx.readthedocs.io/en/latest/getting_started/install.html) to set up your compute environment.\n",
    "Note that you will need to restart your notebook kernel for changes to take effect for running this tutorial.\n",
    "\n",
    "#### Import packages, including icepyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import icepyx as ipx\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import h5py\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import shutil\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set search parameters\n",
    "ICESat-2 data can be searched in several different ways.\n",
    "\n",
    "Here we will search for ATL08 (Land Water Vegetation Elevation) data using a bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_name = 'ATL08' # See https://nsidc.org/data/icesat-2/data-sets for a list of the available datasets.\n",
    "spatial_extent = [-69.7, 44.8, -69.5, 45]\n",
    "date_range = ['2018-01-01','2021-04-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Query object with the desired search parameters\n",
    "\n",
    "We will use these three inputs, `short_name`, `spatial_extent`, and `date_range`, to create an `icepyx.Query` object.\n",
    "`short_name` and `spatial_extent`, the latter of which can be entered as a bounding box, list of coordinates, or by providing a polygon file, are always required.\n",
    "A third input, here `date_range`, is also required.\n",
    "Other potential inputs for this third required input are orbital parameters `cycles` and `tracks`.\n",
    "Additional, optional inputs, such as start and end times and data set version, can also be provided.\n",
    "\n",
    "For more details on the required and optional search criteria and acceptable input formats, please see the [DataAccess example tutorial](https://github.com/icesat2py/icepyx/blob/development/examples/ICESat-2_DAAC_DataAccess_Example.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the data object using our inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a = ipx.Query(short_name, spatial_extent, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatted parameters and function calls allow us to explore the data object we have created.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(region_a.dataset)\n",
    "print(region_a.dates)\n",
    "print(region_a.start_time)\n",
    "print(region_a.end_time)\n",
    "print(region_a.cycles)\n",
    "print(region_a.tracks)\n",
    "print(region_a.dataset_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Built in methods allow us to get more information about our dataset**\n",
    "In addition to viewing the stored object information shown above, we can also request summary information about the dataset itself or confirm that we have manually specified the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.dataset_summary_info()\n",
    "print(region_a.latest_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-order Visualizations\n",
    "\n",
    "There are two visualization methods you can call from your query object.\n",
    "\n",
    "The first is a visualization of your spatial extent on a map.\n",
    "The style of map you will see depends on whether or not you have a certain library, `geoviews`, installed.\n",
    "Under the hood, this is because the `proj` library must be installed with conda (it is not available from PyPI) to support some `geoviews` dependencies.\n",
    "With `geoviews`, this plotting function returns a zoomed-in, interactive map.\n",
    "Otherwise, your spatial extent will plot on a static world map using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.visualize_spatial_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second visualization provides a quick look at the actual ICESat-2 elevations available.\n",
    "The [OpenAltimetry (OA) API](https://openaltimetry.org/data/swagger-ui/#/) provides a nice way to achieve this directly within our workflow.\n",
    "By sending metadata from our `Query` object to the OpenAltimetry API, it can return the most recent (relative to your specified end date) set of elevation data almost instantaneously.\n",
    "\n",
    "**Note: the OA API, and thus this function, currently only supports products `ATL06, ATL07, ATL08, ATL10, ATL12, ATL13`**\n",
    "\n",
    "For additional information on using icepyx's visualiation module, including additional access options and limitations for large spatial extents, please see the [Data Visualization Example tutorial](https://github.com/icesat2py/icepyx/blob/development/examples/ICESat-2_Data_Visualization_Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclemap, rgtmap = region_a.visualize_elevation()\n",
    "cyclemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot elevation for individual RGT**\n",
    "\n",
    "The visualization tool also provides the option to view elevation data by latitude for each ground track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgtmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Order and download the ICESat-2 data\n",
    "\n",
    "For more details on the data ordering, subsetting, and downloading processes, see [ICESat-2_DAAC_DataAccess_Example](https://github.com/icesat2py/icepyx/blob/main/examples/ICESat-2_DAAC_DataAccess_Example.ipynb) and [ICESat-2_DAAC_DataAccess2_Example_Subsetting](https://github.com/icesat2py/icepyx/blob/development/examples/ICESat-2_DAAC_DataAccess2_Subsetting.ipynb).\n",
    "\n",
    "Obtaining data from NSIDC actually occurs in multiple steps, which you are likely familiar with if you've ever used Earthdata Explorer.\n",
    "The first step, which the above visualization used behind the scenes, is to query the data and figure out which granules are a potential match.\n",
    "`icepyx` builds a query for you and submits it to the NSIDC Common Metadata Repository (CMR) to figure out what granules might have data of interest to you.\n",
    "\n",
    "**You can view a list of available granules that match your search criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.avail_granules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to place an order for your data, so that NSIDC will prepare it for download.\n",
    "To order data, you must first **log in to Earthdata**.\n",
    "If you do not have an [Earthdata login](https://urs.earthdata.nasa.gov/), you can easily create one for free.\n",
    "You will be prompted to enter your password, or you can [create a `.netrc` file to store your credentials](https://discourse.pangeo.io/t/earthdata-password-pop-up-box/1358)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthdata_uid = 'icepyx_devteam'\n",
    "email = 'icepyx.dev@gmail.com'\n",
    "region_a.earthdata_login(earthdata_uid, email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're logged in, you can place your data order.\n",
    "See the [Subsetting Example](https://github.com/icesat2py/icepyx/blob/development/examples/ICESat-2_DAAC_DataAccess2_Subsetting.ipynb) for more information on taking advantage of the NSIDC subsetting tools to obtain more manageable files and decrease preprocessing.\n",
    "At this stage, the NSIDC subsetter will actually retrieve your data and crop it to your area of interest.\n",
    "Thus, your order may contain fewer granules (or even no granules!) depending on how your spatial criteria overlap the granule geospatial metadata (see Figure 1 in the [NSIDC Programmatic Access Guide](https://nsidc.org/support/how/how-do-i-programmatically-request-data-services) for some more information on this).\n",
    "\n",
    "Once you **place your order**, `icepyx` will provide updates to let you know how it's progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.order_granules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your order is completed, the files are staged by NSIDC and ready for you to download.\n",
    "\n",
    "**Specify a local data directory and download your granules.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './download'\n",
    "region_a.download_granules(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Open and visualize the downloaded data\n",
    "\n",
    "Now that we have downloaded our data, we can move on to our analysis workflow.\n",
    "\n",
    "A few important notes:\n",
    "1. Ultimately, we plan to generalize these types of reader functions and wrap them into a new class object within `icepyx`. That development is in the planning stages (this would be a great way to get involved, whether or not you've already written/used your own readers). In the meantime, they're included as hard-coded functions in this section of the notebook.\n",
    "2. These functions were modified from those originally written by Bidya Yadav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_atl08(icesat2_path):\n",
    "    \"\"\" Read ATL08 HDF file and return a geodataframe.    \n",
    "        Extract variables of interest and separate the ATL08 file \n",
    "        into each beam (ground track) and ascending/descending orbits.\n",
    "        TODO: Append ascending/descending nodes to output name\n",
    "        \n",
    "        All the hdf files inside the specified folder will be parsed\n",
    "    \"\"\"\n",
    "    files = os.listdir(icesat2_path)\n",
    "    hdf_files = [f for f in files if f.endswith('.h5') and 'ATL08' in f]\n",
    "    print(f'Number of HDF files: {len(hdf_files)}')\n",
    "    atl08_gdf = gpd.GeoDataFrame()\n",
    "    for f in hdf_files:\n",
    "        print(f'Processing HDF file: {f}')\n",
    "        hdf_path = f'{icesat2_path}/{f}'\n",
    "        res_dict = {}\n",
    "        meta_dict = {} #These will hold metadata required for scalars per ground-track\n",
    "        group = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "        qual_str_count = ''\n",
    "        with h5py.File(hdf_path, 'r') as fi:\n",
    "            # subset group based on data\n",
    "            group = [g for g in list(fi.keys()) if g in group]\n",
    "            group1 = len(group)\n",
    "            group = [g for g in group if 'land_segments' in fi[f'/{g}']]\n",
    "            group2 = len(group)\n",
    "            if group2<group1:\n",
    "                print.info(f'Non-empty groups: {group2}/{group1}')\n",
    "            # NB: Assert if at least one group present else may be error due to enumeration\n",
    "            if len(group) == 0:\n",
    "                print(f'No ground track data in this file: {f}')\n",
    "                continue\n",
    "            t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:] #scalar 1 value\n",
    "            for k,g in enumerate(group):\n",
    "                # 1) Read in data for a single beam #\n",
    "                lat = fi[f'/{g}/land_segments/latitude']\n",
    "                lon = fi[f'/{g}/land_segments/longitude']\n",
    "                t_dt = fi[f'/{g}/land_segments/delta_time']\n",
    "                layer_flag = fi[f'/{g}/land_segments/layer_flag']\n",
    "                # Extract everything for terrain and canopy variables\n",
    "                terrain_keys = fi[f'{g}/land_segments/terrain'].keys()\n",
    "                terrain_keys = list(terrain_keys)\n",
    "                terrain_dict = {}\n",
    "                for tk in terrain_keys:\n",
    "                    terrain_dict[tk] = fi[f'{g}/land_segments/terrain/{tk}']\n",
    "                #terrain = pd.DataFrame.from_dict(terrain_dict)\n",
    "                # Do the same for Canopy\n",
    "                canopy_keys = fi[f'{g}/land_segments/canopy'].keys()\n",
    "                canopy_keys = list(canopy_keys)\n",
    "                # Remove these two keys as they contain some multivalue tuples which are just no-data in version 002\n",
    "                canopy_keys.remove('canopy_h_metrics')\n",
    "                canopy_keys.remove('canopy_h_metrics_abs')\n",
    "                canopy_dict = {}\n",
    "                for ck in canopy_keys:\n",
    "                    canopy_dict[ck] = fi[f'{g}/land_segments/canopy/{ck}']                    \n",
    "                # Remove the canopy subset flag that seems added in version 3 because this is an array but we can't save array to shapefile\n",
    "                if 'subset_can_flag' in canopy_dict:\n",
    "                    del canopy_dict['subset_can_flag']\n",
    "                if 'subset_te_flag' in terrain_dict:\n",
    "                    del terrain_dict['subset_te_flag']\n",
    "                #Merge two dictionaries (order should be retained; verify when running again wity Python 3.7)\n",
    "                terrain_dict.update(canopy_dict)\n",
    "                \n",
    "                # 2) To Make Pandas dataframe\n",
    "                # Collect everythin into one dictionary\n",
    "                gt_dict = {'lon':lon, 'lat':lat, 't_dt':t_dt, 'layer_flag':layer_flag}\n",
    "                gt_dict.update(terrain_dict)\n",
    "                #df = pd.DataFrame({'lon':lon, 'lat':lat, 'h_li': h_li, 'q_flag':q_flag, 't_dt':t_dt})\n",
    "                df = pd.DataFrame.from_dict(gt_dict)\n",
    "                nan_value =  fi[f'/{g}/land_segments/terrain/h_te_mean'].attrs['_FillValue']\n",
    "                df= df.replace(nan_value, np.nan)\n",
    "\n",
    "                all_points = len(df)\n",
    "                if len(df)>0:\n",
    "                    # Assemble ground track into a dictionary\n",
    "                    res_dict[g] = df\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # Now that ATL08 data from separate ground tracks are in one dict, merge it to df\n",
    "            if len(res_dict)>0:\n",
    "                # To guard againt empty result dictionary created with no icesat2 passing the quality control\n",
    "                # Combine Dataframes for each of 6 ground-tracks into single Dataframe\n",
    "                count = 0\n",
    "                for k in res_dict.keys():\n",
    "                    if count == 0:\n",
    "                        df = res_dict[k]\n",
    "                        df['strip'] = k\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        df1 = res_dict[k]\n",
    "                        df1['strip'] = k\n",
    "                        df = pd.concat([df, df1], axis=0)\n",
    "                                \n",
    "                df['geometry'] = df[['lon', 'lat']].apply(lambda x: Point(x), axis=1)\n",
    "                gdf = gpd.GeoDataFrame(df, geometry='geometry', crs = 'epsg:4326')\n",
    "\n",
    "                else:\n",
    "                print(f\"\\tNo Ground Track in this HDF file; csv or shp not created\")\n",
    "            \n",
    "            atl08_gdf = atl08_gdf.append(gdf)\n",
    "        \n",
    "        return atl08_gdf         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a geodataframe from the downloaded ATL08 data files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = read_atl08(path+'_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use holoviews to quickly visualize the downloaded data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = gv.tile_sources.ESRI # Add basemap to get sense of where the data is\n",
    "gtracks = gdf.hvplot.points(geo=True, color='strip', s=15, width=500, height=700) # Plot ground tracks\n",
    "# base * gtracks\n",
    "\n",
    "## Plot terrain and canopy data [# Pick one Ground Track ]\n",
    "terrain = gdf[gdf.strip == 'gt2l'].hvplot(y='lat', x='h_te_min', kind='scatter', width=400, height=650, color='brown', s=10, alpha=.7).relabel('Terrain')\n",
    "canopy =  gdf[gdf.strip == 'gt2l'].hvplot(y='lat', x='h_max_canopy_abs', kind='scatter', width=400, height=650, color='green', s=10, alpha=.5, title=f'Elevation', xlabel='meters').relabel('Canopy')\n",
    "\n",
    "# Compose a Plot with with basemap overlaying ground track and another tile with terrain and canopy \n",
    "base * gtracks + terrain * canopy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
